{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "rsLS0yDI5ZHA"
      ],
      "toc_visible": true,
      "mount_file_id": "1VEFS1AOW3YqB8tD5xjqaDfxTE2lWm_il",
      "authorship_tag": "ABX9TyPV9RDKi07ZUmr/GWD526yW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gactyxc/NDCI-mGMM/blob/main/NDCI_mGMM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize"
      ],
      "metadata": {
        "id": "rsLS0yDI5ZHA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGypo1adCGNs"
      },
      "outputs": [],
      "source": [
        "! pip install earthengine-api\n",
        "import ee\n",
        "ee.Authenticate()\n",
        "ee.Initialize(project='crops-mapping-gaoyuan')\n",
        "!pip install geemap\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# global variable defination\n",
        "year = 2020\n",
        "startDoy = 180\n",
        "endDoy = 210\n",
        "roi = ee.Geometry.Rectangle(\n",
        "    coords=[124.0, 44.2, 125.0, 45.2], proj='EPSG:4326', geodesic=False\n",
        ")\n",
        "process_dir = '/content/drive/MyDrive/process'\n",
        "NDCI_samplesFilename = 'testRegion_cropSamples_VI'\n",
        "probabilityMap_fpath = 'testRegion_probabilityMap'"
      ],
      "metadata": {
        "id": "c4iuuXzRgUDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(process_dir):\n",
        "  os.makedirs(process_dir)\n",
        "  print(\"Folder created successfully.\")\n",
        "else:\n",
        "  print(\"Folder already exists.\")\n",
        "ESA_landmap = ee.ImageCollection(\"ESA/WorldCover/v100\")\n",
        "ESA_croplandMask = ESA_landmap.first().eq(40)\n",
        "# filter the sample use cropland mask\n",
        "roi_croplandMask = ESA_croplandMask.clip(roi)\n",
        "roi_croplandMask = roi_croplandMask.updateMask(roi_croplandMask)"
      ],
      "metadata": {
        "id": "74AnWIQlBjfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# part-1: data preprocession"
      ],
      "metadata": {
        "id": "LzblbIk51wLD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# common functions\n",
        "# function to get the sentinel-2 image collection based on the study data range and study area ----*/\n",
        "# function to remove cloud\n",
        "# function to exclude bad data at scene edges\n",
        "def maskEdges(s2_img):\n",
        "    return s2_img.updateMask(\n",
        "        s2_img.select('B8A').mask().updateMask(s2_img.select('B9').mask()))\n",
        "\n",
        "# Function to mask clouds in Sentinel-2 imagery.\n",
        "def maskClouds(img):\n",
        "    max_cloud_probabiltly = 50\n",
        "    clouds = ee.Image(img.get('cloud_mask')).select('probability')\n",
        "    isNotCloud = clouds.lt(max_cloud_probabiltly)\n",
        "    return img.updateMask(isNotCloud)\n",
        "\n",
        "def sentinel2_collection(start_data, end_data, roi):\n",
        "    s2Sr = ee.ImageCollection(\"COPERNICUS/S2_HARMONIZED\")\n",
        "    s2Clouds = ee.ImageCollection(\"COPERNICUS/S2_CLOUD_PROBABILITY\")\n",
        "\n",
        "    # define the filter constraints\n",
        "    criteria = ee.Filter.And(ee.Filter.geometry(roi), ee.Filter.date(start_data, end_data))\n",
        "\n",
        "    # sentinel-2 data collection\n",
        "    sentinel2_bands = ['B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B11', 'B12']\n",
        "    new_bands = ['B', 'G', 'R', 'RE1', 'RE2', 'RE3', 'NIR', 'RE4', 'SWIR1', 'SWIR2']\n",
        "\n",
        "    # Filter input collections by desired data range and region.\n",
        "    s2Sr = s2Sr.filter(criteria).map(maskEdges)\n",
        "    s2Clouds = s2Clouds.filter(criteria)\n",
        "\n",
        "    # Join S2 SR with cloud probability dataset to add cloud mask.\n",
        "    s2SrWithCloudMask = ee.Join.saveFirst('cloud_mask').apply(**{\n",
        "      \"primary\": s2Sr,\n",
        "      \"secondary\": s2Clouds,\n",
        "      \"condition\": ee.Filter.equals(**{\"leftField\": \"system:index\", \"rightField\":\"system:index\"})\n",
        "      })\n",
        "\n",
        "    # collect the images without cloud\n",
        "    s2CloudMasked = ee.ImageCollection(s2SrWithCloudMask).map(maskClouds).select(sentinel2_bands, new_bands)\n",
        "    return s2CloudMasked\n",
        "\n",
        "# function to add all related VIs\n",
        "# add NDCI, a normalized difference composite index for maize identification\n",
        "def addNDCI(img):\n",
        "    NDCI = img.expression('NDCI = 2500 * (10000-SWIR1 - G) / (7.5 * RE1 - SWIR1 + 20000)', {\n",
        "        'RE1': img.select('RE1'),\n",
        "        'G': img.select('G'),\n",
        "        'SWIR1': img.select('SWIR1')\n",
        "    })\n",
        "    img = img.addBands(NDCI)\n",
        "    img = img.toInt16()\n",
        "    return img\n",
        "\n",
        "# add LSWI\n",
        "def addLSWI(image):\n",
        "    LSWI = image.expression('LSWI = 1000 * (NIR - SWIR1) / (NIR + SWIR1)', {\n",
        "        'SWIR1': image.select('SWIR1'),\n",
        "        'NIR': image.select('NIR')\n",
        "    })\n",
        "    return image.addBands(LSWI)\n",
        "\n",
        "# add EVI\n",
        "def addEVI(image):\n",
        "    EVI = image.expression('EVI = 2500 * (NIR - R) / (NIR + 6 * R - 7.5 * B + 10000)', {\n",
        "        'NIR': image.select('NIR'),\n",
        "        'R': image.select('R'),\n",
        "        'B': image.select('B')\n",
        "    })\n",
        "    return image.addBands(EVI)"
      ],
      "metadata": {
        "id": "98jXHfPaieHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# part-2: get NDCI map"
      ],
      "metadata": {
        "id": "BhIdhZDu2Wsd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# year: the target identification year\n",
        "# startDoy: the start Doy of optimal identification window\n",
        "# endDoy: the end Doy of optimal identification window\n",
        "# roi: target region of study\n",
        "# output value of this function is the time series s2sr images in a given time period\n",
        "def get_s2sr_images(year, startDoy, endDoy, roi):\n",
        "    # define the start and end time of identification\n",
        "    startDate = ee.Date.fromYMD(year, 1, 1).advance(startDoy, 'day')\n",
        "    endDate = ee.Date.fromYMD(year, 1, 1).advance(endDoy, 'day')\n",
        "\n",
        "    # define the image collection\n",
        "    s2SR_imgCol = sentinel2_collection(ee.Date.fromYMD(year, 1, 1),\n",
        "                                       ee.Date.fromYMD(year, 12, 31), roi)\n",
        "\n",
        "    # Create a date range list with a specified 10-day interval，use millis as unit\n",
        "    dates = ee.List.sequence(startDate.millis(), endDate.millis(), 1000 * 60 * 60 * 24 * 10)\n",
        "\n",
        "    # function to resample time resolution of image collection to 10 day\n",
        "    def resampleTo10Days(date):\n",
        "        currentDate = ee.Date(date)\n",
        "        endDate = currentDate.advance(10, 'day')\n",
        "        summarizedImageCol = s2SR_imgCol.filterDate(currentDate, endDate)\n",
        "        summarizedImage = summarizedImageCol.median()\n",
        "        summarizedImage = summarizedImage.set('system:time_start', date)\n",
        "        return summarizedImage\n",
        "\n",
        "    # Apply the time resampling function using map()\n",
        "    resampledImages = ee.ImageCollection(dates.map(resampleTo10Days))\n",
        "    resampledImages = resampledImages.map(addLSWI)\n",
        "    resampledImages = resampledImages.map(addEVI)\n",
        "    resampledImages = resampledImages.map(addNDCI)\n",
        "    return resampledImages"
      ],
      "metadata": {
        "id": "dJ0pBWvMDSLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# year: the target identification year\n",
        "# startDoy: the start Doy of optimal identification window\n",
        "# endDoy: the end Doy of optimal identification window\n",
        "# roi: target region of study\n",
        "# output value of this function is the time series NDCI images in a given time period\n",
        "def get_NDCI_map(resampledImages):\n",
        "    # get the BSMI index and the remove the outlier pixels as 0\n",
        "    def NDCI_mask(image):\n",
        "      LSWI = image.select('LSWI')\n",
        "      EVI = image.select('EVI')\n",
        "      mask = EVI.lte(0.35).And(LSWI.add(ee.Image.constant(0.05)).gte(EVI))\n",
        "      valid_mask = mask.Not()\n",
        "      NDCI = image.select('NDCI')\n",
        "      NDCI = NDCI.multiply(valid_mask).rename('NDCI_mask')\n",
        "      NDCI = NDCI.toInt16()\n",
        "      image = image.addBands(NDCI)\n",
        "      return image.select('NDCI_mask')\n",
        "    NDCI_Images = resampledImages.map(NDCI_mask)\n",
        "    return NDCI_Images"
      ],
      "metadata": {
        "id": "IVACJJYE25C1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# part-3: mGMM construction (GEE part)"
      ],
      "metadata": {
        "id": "ggtK8-Ul2lBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------ step-1. get the random sample in each 1°×1° grid\n",
        "# this function use the ESA landMap as cropland mask\n",
        "# roi: target region of study\n",
        "# sampleSize: the random sample size in each grid, the default size is 0.1% of the number of pixels in each grid\n",
        "# output value of this function is random samples in a given grid and given size\n",
        "def get_random_sample(roi, sampleSize=None):\n",
        "\n",
        "    if sampleSize is None:\n",
        "        sampleSize = 124000\n",
        "\n",
        "    randomPoints = ee.FeatureCollection.randomPoints(\n",
        "        region=roi, points=sampleSize, seed=1234, maxError=1\n",
        "        )\n",
        "\n",
        "    def mask_points(point):\n",
        "      isInsideMask = roi_croplandMask.reduceRegion(\n",
        "          reducer=ee.Reducer.first(),\n",
        "          geometry=point.geometry(),\n",
        "          scale=10,\n",
        "          maxPixels=1\n",
        "      ).getNumber('Map')\n",
        "      return point.set('inside_mask', isInsideMask)\n",
        "\n",
        "    maskedPoints = randomPoints.map(mask_points)\n",
        "    finalPoints = maskedPoints.filter(ee.Filter.eq('inside_mask', 1))\n",
        "    return finalPoints"
      ],
      "metadata": {
        "id": "z1OxPSQMjRzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#------ step-2. extract the NDCI value of each image for each random point\n",
        "# imgCol: the extracted time series images\n",
        "# pts: extraction points\n",
        "# this funtion runs to derive the image values of given samples\n",
        "def extract_points_value(imgCol, pts):\n",
        "  ft = ee.FeatureCollection(ee.List([]))\n",
        "\n",
        "  def fill(img, ini):\n",
        "    date = ee.Date(img.date()).format()\n",
        "    inift = ee.FeatureCollection(ini)\n",
        "    ft2 = img.sampleRegions(\n",
        "        collection = pts,\n",
        "        properties = ['ID'], # Properties to include from points\n",
        "        scale = 10\n",
        "    )\n",
        "    ft3 = ft2.map(lambda f: f.set('date', date))\n",
        "    return inift.merge(ft3)\n",
        "  newft = ee.FeatureCollection(imgCol.iterate(fill, ft))\n",
        "  task = ee.batch.Export.table.toDrive(\n",
        "      collection=newft,\n",
        "      description = NDCI_samplesFilename,\n",
        "      folder = 'process',\n",
        "      fileFormat = 'CSV'\n",
        "  )\n",
        "  task.start()"
      ],
      "metadata": {
        "id": "-aTqFCYledae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%% main procedure\n",
        "# part-1: get_NDCI_samples\n",
        "imgCol = get_s2sr_images(year, startDoy, endDoy, roi)\n",
        "NDCI_imgCol = get_NDCI_map(imgCol)\n",
        "pts = get_random_sample(roi)\n",
        "extract_points_value(NDCI_imgCol, pts)"
      ],
      "metadata": {
        "id": "kUrqnxq9WRfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# part-3: mGMM construction (python part)"
      ],
      "metadata": {
        "id": "LcvfTOvz5E1T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install astropy"
      ],
      "metadata": {
        "id": "iO1hE3AKYpd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- step-1: separate the sample data by DOY\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from astropy.time import Time\n",
        "\n",
        "def separate_samples(file_path, doy_file_dir):\n",
        "  f = open(file_path, 'r', encoding=\"utf-8\")\n",
        "  primary_data = pd.read_csv(f)\n",
        "  data = primary_data.drop(['system:index','.geo'], axis=1)\n",
        "  # get all DOY\n",
        "  date = data['date'].values.tolist()\n",
        "  data_time = Time(date,format='isot', scale='utc')\n",
        "  start_time = Time(f'{year}-01-01', format='isot', scale='utc')\n",
        "  data_doy = data_time.jd-start_time.jd\n",
        "  data['doy'] = data_doy\n",
        "  doy_list = np.unique(data_doy)\n",
        "  if not os.path.exists(doy_file_dir):\n",
        "    os.mkdir(doy_file_dir)\n",
        "    for doy in doy_list:\n",
        "      doy_str = str(int(doy))\n",
        "      out_filename = doy_file_dir +'/'+ doy_str + '.csv'\n",
        "      temp_data = data.loc[(data.doy == doy),:]\n",
        "      out_data = temp_data['NDCI_mask']\n",
        "      out_data.to_csv(out_filename, mode='a', index=False, header=False)\n",
        "  print('sepatrate sample data down.')"
      ],
      "metadata": {
        "id": "5ZcayTLHTJCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install scikit-learn\n",
        "!pip install scipy"
      ],
      "metadata": {
        "id": "GTkZdJxD7DvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- step-2: multi-temporal GMM construction\n",
        "from scipy.stats import multivariate_normal\n",
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "# function to removed the outlier samples based on the IQR\n",
        "def remove_outlier_samples_IQR(sampled_pixels):\n",
        "    Q1 = np.quantile(sampled_pixels,0.25)\n",
        "    Q3 = np.quantile(sampled_pixels,0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    min_range = Q1 - 1.5 * IQR\n",
        "    max_range = Q3 + 1.5 * IQR\n",
        "    filtered_pixels = sampled_pixels[(sampled_pixels >= min_range) & (sampled_pixels <= max_range)]\n",
        "    return filtered_pixels\n",
        "# determine whether the value is within the given interval\n",
        "def if_in_range(x_arr,range_left,range_right):\n",
        "    root = np.nan\n",
        "    for x in x_arr:\n",
        "        if (x > range_left and x < range_right):\n",
        "            root = x\n",
        "    return root\n",
        "# function to calculate the OLR of the Gaussian Mixture Model\n",
        "def calculate_OLR(mu,cov,pi):\n",
        "    # get the intersection point of two guassian curve\n",
        "    sigma1 = np.sqrt(cov[0])\n",
        "    mu1 = mu[0]\n",
        "    pi1 = pi[0]\n",
        "    sigma2 = np.sqrt(cov[1])\n",
        "    mu2 = mu[1]\n",
        "    pi2 = pi[1]\n",
        "    a = sigma1 ** 2 - sigma2 ** 2\n",
        "    b = 2 * (sigma2 ** 2 * mu1 - sigma1 ** 2 * mu2)\n",
        "    c = sigma1 ** 2 * mu2 ** 2 - sigma2 ** 2 * mu1 ** 2 - 2 *sigma1 ** 2 * sigma2 ** 2 * np.log((sigma1 * pi2)/ (sigma2 * pi1))\n",
        "    p = np.poly1d([a[0][0], b[0][0], c[0][0]])\n",
        "    g1 = multivariate_normal(mean=mu[0], cov=cov[0])\n",
        "    g2 = multivariate_normal(mean=mu[1], cov=cov[1])\n",
        "    p1 = pi[0]*g1.pdf(mu[0])\n",
        "    p2 = pi[1]*g2.pdf(mu[1])\n",
        "    p_subMax = min(p1,p2)\n",
        "    intersections_x = p.r\n",
        "    if mu[0] < mu[1]:\n",
        "        root = if_in_range(intersections_x,mu[0],mu[1]) # intersection_x\n",
        "    else:\n",
        "        root = if_in_range(intersections_x,mu[1],mu[0]) # intersection_x\n",
        "    if (~np.isnan(root)):\n",
        "        p_saddle = pi[0]*g1.pdf(root) # intersection_y\n",
        "        OLR = p_saddle / p_subMax # get the overlap rate (OLR)\n",
        "    else:\n",
        "        OLR = 1\n",
        "    return OLR\n",
        "\n",
        "def Multi_GMM_construction(samples_dir):\n",
        "    samples_list = os.listdir(samples_dir)\n",
        "    OLR_df=pd.DataFrame()\n",
        "    for file in samples_list:\n",
        "        doy = file.split('.')[0]\n",
        "        inputfile = samples_dir +'/'+ file\n",
        "        f = open(inputfile, 'r', encoding=\"utf-8\")\n",
        "        data = pd.read_csv(f)\n",
        "        all_data = data.values.reshape(-1,1)\n",
        "        data = all_data[all_data != 0].reshape(-1,1)\n",
        "        X_train = remove_outlier_samples_IQR(data) # remove the outlier samples using IQR method\n",
        "        gmm = GaussianMixture(n_components=2)  # constructe the GMM model with 2 cluster\n",
        "        gmm.fit(X_train.reshape(-1,1))\n",
        "        means = gmm.means_\n",
        "        covs = gmm.covariances_\n",
        "        weights = gmm.weights_\n",
        "        OLR = calculate_OLR(means,covs,weights)\n",
        "        #columns = ['DOY','mean_0','mean_1','cov_0','cov_1','weight_0','weight_1','OLR']\n",
        "        if means[0][0] < means[1][0]:\n",
        "            parameters = np.array([means[0][0],means[1][0],covs[0][0][0],covs[1][0][0],weights[0],weights[1],OLR])\n",
        "        else:\n",
        "            parameters = np.array([means[1][0],means[0][0],covs[1][0][0],covs[0][0][0],weights[1],weights[0],OLR])\n",
        "        OLR_df[doy] = parameters\n",
        "    OLRs = OLR_df.loc[6]\n",
        "    print('all doy calculation down!')\n",
        "    return OLR_df"
      ],
      "metadata": {
        "id": "R1oV1_fH5l63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# step-3: get the GMM variables of each DOY based on the NDCI_samplesFilename\n",
        "# check whether get_NDCI_samples task run out\n",
        "import os\n",
        "import time\n",
        "samples_file_path = process_dir + '/' + NDCI_samplesFilename + '.csv'\n",
        "doy_file_dir = process_dir + '/doy_samples'\n",
        "\n",
        "while True:\n",
        "  if os.path.exists(samples_file_path):\n",
        "    print(\"Task run out!\")\n",
        "    with open(samples_file_path,'r') as file:\n",
        "      # get the GMM variables of each DOY based on the NDCI_samplesFilename\n",
        "      separate_samples(samples_file_path,doy_file_dir)\n",
        "      GMM_variables = Multi_GMM_construction(doy_file_dir)\n",
        "      print(GMM_variables)\n",
        "    break;\n",
        "  else:\n",
        "    print(\"File not found. Waiting for 30 seconds...\")\n",
        "    time.sleep(30) # wait 30 seconds for next checking"
      ],
      "metadata": {
        "id": "zpLOEDN7DTLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# part-4: NDCI-mGMM implementation in GEE"
      ],
      "metadata": {
        "id": "kYICccSn7tK5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define the mean, cov, weight and OLR\n",
        "means_0 = GMM_variables.iloc[0,:].values\n",
        "means_1 = GMM_variables.iloc[1,:].values\n",
        "covs_0 = GMM_variables.iloc[2,:].values\n",
        "covs_1 = GMM_variables.iloc[3,:].values\n",
        "weights_0 = GMM_variables.iloc[4,:].values\n",
        "weights_1 = GMM_variables.iloc[5,:].values\n",
        "OLRs = GMM_variables.iloc[6,:].values\n",
        "\n",
        "# define the imageCollection for GMM implementation\n",
        "imgCol_list = imgCol.toList(imgCol.size())\n",
        "n = imgCol_list.size().getInfo()\n",
        "index_list = list(range(0, n))\n",
        "\n",
        "# function to get the GMM probability of a given single image\n",
        "def get_probability(index):\n",
        "  image = ee.Image(imgCol_list.get(index))\n",
        "  # mask by the ESA cropland mask\n",
        "  image = image.updateMask(roi_croplandMask)\n",
        "  # get the pdf value of each cluster based on the GMM\n",
        "  pdf_1 = image.expression(\n",
        "      '(wi / (sqrt(2 * 3.14159265359 * cov))) * exp(-((x - mean) * (x - mean) / (2 * cov)))', {\n",
        "        'x': image.select('NDCI'),\n",
        "        'mean': means_0[index],\n",
        "        'cov': covs_0[index],\n",
        "        'wi': weights_0[index]\n",
        "    })\n",
        "  pdf_2 = image.expression(\n",
        "      '(wi / (sqrt(2 * 3.14159265359 * cov))) * exp(-((x - mean) * (x - mean) / (2 * cov)))', {\n",
        "        'x': image.select('NDCI'),\n",
        "        'mean': means_1[index],\n",
        "        'cov': covs_1[index],\n",
        "        'wi': weights_1[index]\n",
        "    })\n",
        "  probability = pdf_2.divide(pdf_1.add(pdf_2)).rename('maize_prop')\n",
        "\n",
        "  # get the mask of outlier NDCI pixel rule\n",
        "  NDCI_mask = ((image.select('EVI').lte(0.35)).And\n",
        "             ((image.select('LSWI').add(ee.Image.constant(0.05))).gte(image.select('EVI')))).Not().rename('NDCI_mask')\n",
        "\n",
        "  image = image.addBands(NDCI_mask)\n",
        "\n",
        "  # Mask the probability to 0 with the mask\n",
        "  probability = probability.updateMask(NDCI_mask)\n",
        "  # Get the probability weight of each image based on the OLR\n",
        "  OLR = OLRs[index]\n",
        "  OLR_wi = ee.Image.constant(1 - OLR).rename('OLR_1')\n",
        "  OLR_wi = OLR_wi.where(NDCI_mask.eq(0), 1)\n",
        "  OLR_wi = OLR_wi.toFloat()\n",
        "\n",
        "  image = image.addBands(probability)\n",
        "  image = image.addBands(OLR_wi)\n",
        "  return image\n",
        "\n",
        "# get maize probability of each single image\n",
        "prob_Collection = ee.ImageCollection.fromImages([get_probability(index) for index in index_list])\n",
        "print('prob_Collection bandnames',prob_Collection.first().bandNames().getInfo())\n",
        "\n",
        "sum_OLRwi = prob_Collection.select('OLR_1').reduce(ee.Reducer.sum())\n",
        "prob_list = prob_Collection.toList(prob_Collection.size())\n",
        "final_MaizeProb = ee.Image.constant(0)\n",
        "\n",
        "for doy_num in index_list:\n",
        "    index_image = ee.Image(prob_list.get(doy_num))\n",
        "    BSMI_mask = index_image.select('NDCI_mask')\n",
        "    probability = index_image.select('maize_prop')\n",
        "    probability = probability.unmask(0)\n",
        "    OLR_1 = index_image.select('OLR_1')\n",
        "    prob_weight = OLR_1.divide(sum_OLRwi)\n",
        "    MaizeProb = probability.multiply(prob_weight)\n",
        "    MaizeProb = MaizeProb.unmask(0)\n",
        "    final_MaizeProb = final_MaizeProb.add(MaizeProb)\n",
        "\n",
        "final_MaizeProb = final_MaizeProb.multiply(1000).toInt16()\n",
        "\n",
        "# export the probability map to drive\n",
        "task = ee.batch.Export.image.toDrive(\n",
        "    image=final_MaizeProb.clip(roi),\n",
        "    description = probabilityMap_fpath,\n",
        "    folder = 'process',\n",
        "    fileNamePrefix = probabilityMap_fpath,\n",
        "    region = roi,\n",
        "    scale = 10,\n",
        "    crs = \"EPSG:4326\",\n",
        "    maxPixels = 1e13\n",
        ")\n",
        "task.start()"
      ],
      "metadata": {
        "id": "Arf_58_a7YcR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# main procedure"
      ],
      "metadata": {
        "id": "W02D719Q3hRQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import geemap\n",
        "\n",
        "m = geemap.Map()\n",
        "m.set_center(124.5, 44.7, 7)\n",
        "m.add_layer(roi, {'color': 'yellow'}, 'Region')\n",
        "m.add_basemap('SATELLITE')\n",
        "m.add_layer(final_MaizeProb.clip(roi),{'min': 0,'max': 1000,'palette': ['gray', 'orange']},'final_maizeMap')\n",
        "m"
      ],
      "metadata": {
        "id": "fTs7YOGOr2Z2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}